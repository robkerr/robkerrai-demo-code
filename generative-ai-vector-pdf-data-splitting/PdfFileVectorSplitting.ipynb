{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Libraries and Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries into kernel (if not already installed)\n",
    "# %pip install pinecone-client\n",
    "# %pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Python data handling environment imports \n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "# Pinecone is a cloud-based Vector Database we'll use \n",
    "# to store embeddings\n",
    "import pinecone\n",
    "\n",
    "# OpenAI is used for the embedding LLM and GenAI model \n",
    "# used to generate responses\n",
    "import openai\n",
    "\n",
    "# Langchain is middleware that ties together the components \n",
    "# of the embedding and retrieval pipelines \n",
    "\n",
    "# The embedding chain creates searchable vectors of our data\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "# from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "# A link in the chain to operate a chat session\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# We'll maintain some memory of the chat so follow-up questions\n",
    "# will be context-sensitive\n",
    "from langchain.chains.conversation.memory \\\n",
    "import ConversationBufferWindowMemory\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Environment Variables\n",
    "\n",
    "When using VSCode, install the dotenv extension and create an .env file with these contents:\n",
    "\n",
    "OPENAI_KEY=YOUR_OPENAI_API_KEY\n",
    "\n",
    "PINECONE_KEY=YOUR_PINECONE_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_KEY=os.getenv(\"OPENAI_KEY\")\n",
    "openai.api_key = OPENAI_KEY\n",
    "EMBEDDING_MODEL=\"text-embedding-ada-002\"\n",
    "GENAI_MODEL='gpt-3.5-turbo'\n",
    "\n",
    "PINECONE_KEY=os.getenv(\"PINECONE_KEY\")\n",
    "PINECONE_ENV=\"gcp-starter\"\n",
    "PINECONE_INDEX_NAME=\"default\" # this will be created below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Pinecone Vector Database if does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(api_key = PINECONE_KEY, environment = PINECONE_ENV)\n",
    "index_list = pinecone.list_indexes()\n",
    "if len(index_list) == 0:\n",
    "    print(\"Creating index...\")\n",
    "    pinecone.create_index(PINECONE_INDEX_NAME, dimension=1536, metric='dotproduct')\n",
    "    \n",
    "print(pinecone.describe_index(PINECONE_INDEX_NAME))\n",
    "index = pinecone.Index(PINECONE_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embedding Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This references the text-embedding-ada-002 OpenAI model we'll use to create embeddings \n",
    "# Both for indexing ground knowledge content, and later when searching ground knowledge\n",
    "# For RAG documents to include in LLM Prompts\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model = EMBEDDING_MODEL,\n",
    "    openai_api_key= OPENAI_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function to Split PDF File into Vectors & UPSERT vectors to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_db(file):   \n",
    "    print(f\"Splitting and vectorizing file: {file}\")\n",
    "    \n",
    "    # load document from file disk\n",
    "    loader = PyPDFLoader(file)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # split documents into text and embeddings\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # Vectorize chunks of file, submitting 20 chunks at a time to OpenAI\n",
    "    batch_size = 20 \n",
    "    for i in tqdm(range(0, len(docs), batch_size)):\n",
    "        # OpenAPI has rate limits, and we use batches to slow the pace of embedding requests\n",
    "        i_end = min(i+batch_size, len(docs))\n",
    "        batch = docs[i:i_end]\n",
    "        \n",
    "        # When querying the Vector DB for nearest vectors, the metadata \n",
    "        # is what is returned and added to the LLM Prompt (the \"Grounding Knowledge\")\n",
    "        ids = []\n",
    "        context_array = []\n",
    "        meta_data = []\n",
    "        for i, row in enumerate(batch):\n",
    "            print(f\"appending {i}\")\n",
    "            # Create a UUID\n",
    "            ids.append(str(uuid.uuid4()))\n",
    "            context_array.append(row.page_content)\n",
    "            meta_data.append({\n",
    "                'source': row.metadata[\"source\"],\n",
    "                'page': row.metadata[\"page\"] + 1,\n",
    "                'context': row.page_content\n",
    "            })            \n",
    "        \n",
    "        # print(ids)\n",
    "        # print(meta_data)\n",
    "        \n",
    "        \n",
    "        # Get a list of documents to submit to OpenAI for embedding  \n",
    "        emb_vectors = embed.embed_documents(context_array) \n",
    "        \n",
    "        # Add embeddings, associated metadata, and the keys to the vector DB\n",
    "        to_upsert = zip(id, emb_vectors, meta_data)    \n",
    "        index.upsert(vectors=to_upsert)\n",
    "\n",
    "    \n",
    "        # Pause after each batch to avoid rate limits\n",
    "        time.sleep(2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_db('files/2019-21-51_Emergency.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a simple query to the Vector Index to ensure we it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Pinecone(index, embed, \"context\")\n",
    "query = \"What model aircraft is affected by directive 2019-21-51?\" #ask some question that's answerable with the content added to the Vector DB\n",
    "vectorstore.similarity_search(query, k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a GPT 3.5 Turbo Chatbot with a 5 response memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reference to the OpenAI LLM\n",
    "llm = ChatOpenAI(openai_api_key = OPENAI_KEY,\n",
    "                model_name = GENAI_MODEL,\n",
    "                temperature = 0.0)\n",
    "\n",
    "# Ensure the chat session includes memory of 5 previous messages\n",
    "conv_mem = ConversationBufferWindowMemory(\n",
    "    memory_key = 'history',\n",
    "    k = 5,\n",
    "    return_messages =True)\n",
    "\n",
    "# Create the chain to manage the chat session\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\",\n",
    "    retriever = vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now have a conversation about the documents that were added to the grounding data vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.run(\"What model aircraft is affected by directive 2019-21-51?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.run(\"what are the unsafe conditions? Format as a bulleted list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.run(\"Does dell make surfboards?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.run(\"Do they make laptops?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.run(\"Who founded Dell computer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
