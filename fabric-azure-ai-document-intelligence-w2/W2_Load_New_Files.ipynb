{"cells":[{"cell_type":"markdown","source":["# This workbook imports W2 Data from Scanned W2 Documents\n","\n","Note: this notebook is for demonstration/proof-of-concept usage and isn't built for production scale. \n","\n","- Submissions to Azure AI Document Intelligence service wait for each completion (polling). In production a production solution, submissions should be submitted and results read in a separate process.\n","- Little to no error checking or validation is done to keep the code more clear and easy to follow.\n","\n","The source of the input W2 forms is a set of Fake W2 documents [available on Kaggle](https://www.kaggle.com/datasets/mcvishnu1/fake-w2-us-tax-form-dataset)\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"9f96285f-2d73-4073-9bfa-f3e7cb2794ea"},{"cell_type":"code","source":["%pip install azure-ai-documentintelligence\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"7b3b51e5-d47f-46cc-b7e5-d09db64bc552"},{"cell_type":"markdown","source":["### Get Key for Azure AI Services from Key Vault"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f6cf98ec-b927-40ab-aa37-712802b83f04"},{"cell_type":"code","source":["# Get Azure AI Services Keys\n","from trident_token_library_wrapper \\\n","import PyTridentTokenLibrary as tl\n","\n","key_vault_name = 'designmind-fabric-ai'\n","key_name = \"AZURE-AI-SERVICES-KEY\" \n","ai_services_endpoint = \"https://rhk-demo-aiservices.cognitiveservices.azure.com/\"\n","\n","# Get access token to key vault for current session ID\n","access_token = mssparkutils.credentials.getToken(\"keyvault\")\n","\n","# Get secret value from Key Vault using the access token\n","ai_services_key = tl.get_secret_with_token( \\\n","  f\"https://{key_vault_name}.vault.azure.net/\", \\\n","  key_name, \\\n","  access_token)\n","ai_services_region = \"eastus\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"35f9dffe-c7ce-4050-a068-dd8e1c3ab94e","statement_id":24,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-06T18:08:56.5444853Z","session_start_time":null,"execution_start_time":"2024-01-06T18:08:56.8984501Z","execution_finish_time":"2024-01-06T18:08:57.2166268Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"25f14e2c-7ea7-4b4d-a09e-1869bba176aa"},"text/plain":"StatementMeta(, 35f9dffe-c7ce-4050-a068-dd8e1c3ab94e, 24, Finished, Available)"},"metadata":{}}],"execution_count":12,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"47d866fd-bf8e-4982-b431-641c3e21a971"},{"cell_type":"code","source":["print(ai_services_key)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"35f9dffe-c7ce-4050-a068-dd8e1c3ab94e","statement_id":15,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-06T18:02:46.2828219Z","session_start_time":null,"execution_start_time":"2024-01-06T18:02:46.5909168Z","execution_finish_time":"2024-01-06T18:02:46.9033447Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"badf71ac-896a-40da-a079-41cc382d5981"},"text/plain":"StatementMeta(, 35f9dffe-c7ce-4050-a068-dd8e1c3ab94e, 15, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[REDACTED]\n"]}],"execution_count":8,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"167f3b2c-ac04-434d-8e0a-43c0ca86dbe8"},{"cell_type":"code","source":["# Source read for new scanned forms W2 in JPG format\n","source_folder = \"Files/W2_Scanned_Images/New_Files\"\n","source_folder_file_api = \"/lakehouse/default/Files/W2_Scanned_Images/New_Files\"\n","\n","# Processed files are moved to an Archive folder for reference\n","archive_folder = \"/lakehouse/default/Files/W2_Scanned_Images/Loaded_Archive\"\n","\n","# The output Delta table where extracted data is appended\n","delta_table_name = \"Forms_W2\"\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"35f9dffe-c7ce-4050-a068-dd8e1c3ab94e","statement_id":25,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-06T18:09:01.6641073Z","session_start_time":null,"execution_start_time":"2024-01-06T18:09:02.0127052Z","execution_finish_time":"2024-01-06T18:09:02.3175035Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"8ee53d50-3e48-4a97-8951-faff694b0601"},"text/plain":"StatementMeta(, 35f9dffe-c7ce-4050-a068-dd8e1c3ab94e, 25, Finished, Available)"},"metadata":{}}],"execution_count":13,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"4d7f1d1e-6ab6-4db2-9fd3-64b12fee64fb"},{"cell_type":"markdown","source":["### Define the structure of the output Delta Table"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"651c7ad3-a8c0-4a63-b394-69dc072885cc"},{"cell_type":"code","source":["from pyspark.sql.types import StructType,StructField, StringType, BooleanType, DecimalType\n","\n","schema = StructType([ \n","    StructField(\"scanned_filename\",StringType(), True),\n","    StructField(\"form_variant\",StringType(),True), \n","    StructField(\"tax_year\",StringType(),True),    \n","    StructField(\"w2_copy\", StringType(), True),\n","    StructField(\"control_number\", StringType(), True), \n","    StructField(\"employee_name\",StringType(),True),   \n","    StructField(\"employee_ssn\", StringType(), True),\n","    StructField(\"employee_street\", StringType(), True),\n","    StructField(\"employee_city\", StringType(), True), \n","    StructField(\"employee_state\", StringType(), True), \n","    StructField(\"employee_postal_code\", StringType(), True), \n","    StructField(\"employer_name\", StringType(), True),\n","    StructField(\"employer_id\", StringType(), True),\n","    StructField(\"employer_street\", StringType(), True),\n","    StructField(\"employer_city\", StringType(), True), \n","    StructField(\"employer_state\", StringType(), True), \n","    StructField(\"employer_postal_code\", StringType(), True), \n","    StructField(\"wages_tips\", DecimalType(10,2), True),\n","    StructField(\"fed_income_tax_withheld\", DecimalType(10,2), True),\n","    StructField(\"social_security_wages\", DecimalType(10,2), True),\n","    StructField(\"social_security_tax_withheld\", DecimalType(10,2), True),\n","    StructField(\"medicare_wages_tips\", DecimalType(10,2), True),\n","    StructField(\"medicare_tax_withheld\", DecimalType(10,2), True),\n","    StructField(\"social_security_tips\", DecimalType(10,2), True),\n","    StructField(\"allocated_tips\", DecimalType(10,2), True),\n","    StructField(\"non_qualified_plans\", DecimalType(10,2), True),\n","    StructField(\"dependent_care_benefits\", DecimalType(10,2), True),\n","    StructField(\"is_statutory_employee\", BooleanType(), True),\n","    StructField(\"is_retirement_plan\", BooleanType(), True),\n","    StructField(\"is_third_party_sick_pay\", BooleanType(), True)\n","  ])"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"35f9dffe-c7ce-4050-a068-dd8e1c3ab94e","statement_id":26,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-06T18:09:05.6400622Z","session_start_time":null,"execution_start_time":"2024-01-06T18:09:05.9660432Z","execution_finish_time":"2024-01-06T18:09:06.3495809Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"3dcbf6fe-14b7-4626-8cb1-a7a421713a28"},"text/plain":"StatementMeta(, 35f9dffe-c7ce-4050-a068-dd8e1c3ab94e, 26, Finished, Available)"},"metadata":{}}],"execution_count":14,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"74aee169-cd9f-4c71-8857-8c0c2103ff0c"},{"cell_type":"markdown","source":["### Use Azure AI Document Intelligence to Extract Information from a scanned W2 image"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"35dd3f62-af69-4aca-a664-6f9ca43f1ef2"},{"cell_type":"code","source":["def analyze_tax_us_w2(filename, blob):\n","\n","    from azure.core.credentials import AzureKeyCredential\n","    from azure.ai.documentintelligence import DocumentIntelligenceClient \n","    from decimal import Decimal\n","\n","    document_intelligence_client = DocumentIntelligenceClient(endpoint=ai_services_endpoint, credential=AzureKeyCredential(ai_services_key))\n","\n","    print(\"Polling for Azure AI response...\")\n","    poller = document_intelligence_client.begin_analyze_document(\"prebuilt-tax.us.w2\", blob)\n","    w2s = poller.result()\n","    print(\"...have AI response!\")\n","\n","    output = []\n","\n","    for idx, w2 in enumerate(w2s.documents):\n","        \n","        json = {}\n","        json[\"scanned_filename\"] = filename\n","\n","        # add field to JSON object if it exists\n","        def get_field(container, docField, jsonField, dataType):\n","            # Note: obj.get('valueString') fetches value; obj.confidence fetches 0-1 confidence \n","            if type(container) is dict:\n","                obj = container.get(docField)\n","                if obj:\n","                    json[jsonField] = obj.get(dataType)\n","            else:\n","                obj = container.fields.get(docField)\n","                if obj:\n","                    if dataType == \"valueNumber\":\n","                        json[jsonField] = Decimal(obj.get(dataType))\n","                    else:\n","                        strVal = obj.get(dataType)\n","                        match strVal:\n","                            case \"true\":\n","                                json[jsonField] = True\n","                            case \"false\":\n","                                json[jsonField] = False\n","                            case _:\n","                                json[jsonField] = strVal\n","                        \n","        # extract employee address info\n","        def get_address(container, prefix):\n","            address = container.get(\"Address\")\n","            if address:\n","                valueAddress = address.get(\"valueAddress\")\n","                json[f'{prefix}_street'] = f\"{valueAddress.house_number} {valueAddress.road}\"\n","                json[f'{prefix}_city'] = valueAddress.city\n","                json[f'{prefix}_state'] = valueAddress.state\n","                json[f'{prefix}_postal_code'] = valueAddress.get(\"postalCode\")\n","        \n","        get_field(w2, \"W2FormVariant\", \"form_variant\", 'valueString')\n","        get_field(w2, \"TaxYear\", \"tax_year\", 'valueString')\n","        get_field(w2, \"W2Copy\", \"w2_copy\", 'valueString')\n","        get_field(w2, \"ControlNumber\", \"control_number\", 'valueString')\n","\n","        employee = w2.fields.get(\"Employee\")\n","        if employee:\n","            obj = employee.get(\"valueObject\")\n","            get_field(obj, \"Name\", \"employee_name\", \"valueString\")\n","            get_field(obj, \"SocialSecurityNumber\", \"employee_ssn\", \"valueString\")\n","            get_address(obj, \"employee\")\n","                \n","        employer = w2.fields.get(\"Employer\")\n","        if employer:\n","            obj = employer.get(\"valueObject\")\n","            get_field(obj, \"Name\", \"employer_name\", \"valueString\")\n","            get_field(obj, \"IdNumber\", \"employer_id\", \"valueString\")\n","            get_address(obj, \"employer\")\n","\n","        get_field(w2, \"WagesTipsAndOtherCompensation\", \"wages_tips\", \"valueNumber\")\n","        get_field(w2, \"FederalIncomeTaxWithheld\", \"fed_income_tax_withheld\", \"valueNumber\")\n","        get_field(w2, \"SocialSecurityWages\", \"social_security_wages\", \"valueNumber\")\n","        get_field(w2, \"SocialSecurityTaxWithheld\", \"social_security_tax_withheld\", \"valueNumber\")\n","        get_field(w2, \"MedicareWagesAndTips\", \"medicare_wages_tips\", \"valueNumber\")\n","        get_field(w2, \"MedicareTaxWithheld\", \"medicare_tax_withheld\", \"valueNumber\")\n","        get_field(w2, \"SocialSecurityTips\", \"social_security_tips\", \"valueNumber\")\n","        get_field(w2, \"AllocatedTips\", \"allocated_tips\", \"valueNumber\")\n","        get_field(w2, \"VerificationCode\", \"verification_code\", \"valueNumber\")\n","        get_field(w2, \"DependentCareBenefits\", \"dependent_care_benefits\", \"valueNumber\")\n","        get_field(w2, \"NonQualifiedPlans\", \"non_qualified_plans\", \"valueNumber\")\n","        get_field(w2, \"IsStatutoryEmployee\", \"is_statutory_employee\", \"valueString\")\n","        get_field(w2, \"IsRetirementPlan\", \"is_retirement_plan\", \"valueString\")\n","        get_field(w2, \"IsThirdPartySickPay\", \"is_third_party_sick_pay\", \"valueString\")\n","        get_field(w2, \"Other\", \"other_info\", \"valueString\")\n","\n","        output.append(json)\n","\n","    return output"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"35f9dffe-c7ce-4050-a068-dd8e1c3ab94e","statement_id":27,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-06T18:09:09.1483178Z","session_start_time":null,"execution_start_time":"2024-01-06T18:09:09.4486117Z","execution_finish_time":"2024-01-06T18:09:09.7590976Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"0d48b4f0-07ae-45be-9357-316cb8d4f7fd"},"text/plain":"StatementMeta(, 35f9dffe-c7ce-4050-a068-dd8e1c3ab94e, 27, Finished, Available)"},"metadata":{}}],"execution_count":15,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"5a78b139-55af-4ec2-b47f-d680f0beb4bb"},{"cell_type":"markdown","source":["### Save a batch of W2 Forms to a Delta Table"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"2a99c999-14f8-4b0c-9d08-9926f651eaeb"},{"cell_type":"code","source":["# Transform array of JSON objects to array of Tuples expected by PySpark\n","\n","\n","def save_batch_to_table(json_objects):\n","    print(f\"Writing {len(json_objects)} rows to Delta table.\")\n","    rows = []\n","\n","    for obj in json_objects:\n","        rows.append((obj[\"scanned_filename\"],\n","            obj[\"form_variant\"], obj[\"tax_year\"], \\\n","            obj[\"w2_copy\"], obj[\"control_number\"], \\\n","            obj[\"employee_name\"], obj[\"employee_ssn\"], \\\n","            obj[\"employee_street\"], obj[\"employee_city\"], \\\n","            obj[\"employee_state\"], obj[\"employee_postal_code\"], \\\n","            obj[\"employer_name\"], obj[\"employer_id\"], \\\n","            obj[\"employer_street\"], obj[\"employer_city\"], \\\n","            obj[\"employer_state\"], obj[\"employer_postal_code\"], \\\n","            obj[\"wages_tips\"], obj[\"fed_income_tax_withheld\"], \\\n","            obj[\"social_security_wages\"], obj[\"social_security_tax_withheld\"], \\\n","            obj[\"medicare_wages_tips\"], obj[\"medicare_tax_withheld\"], \\\n","            obj[\"social_security_tips\"], obj[\"allocated_tips\"], \\\n","            obj[\"non_qualified_plans\"], obj[\"dependent_care_benefits\"], \\\n","            obj[\"is_statutory_employee\"], obj[\"is_retirement_plan\"], \\\n","            obj[\"is_third_party_sick_pay\"]\n","        ))\n","\n","    df = spark.createDataFrame(data=rows, schema=schema)\n","    df.write.mode(\"append\").format(\"delta\").saveAsTable(delta_table_name)\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"35f9dffe-c7ce-4050-a068-dd8e1c3ab94e","statement_id":28,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-06T18:09:13.0390663Z","session_start_time":null,"execution_start_time":"2024-01-06T18:09:13.4000471Z","execution_finish_time":"2024-01-06T18:09:13.7125382Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"415def97-02cc-4141-8ec0-5bcb4d8fd270"},"text/plain":"StatementMeta(, 35f9dffe-c7ce-4050-a068-dd8e1c3ab94e, 28, Finished, Available)"},"metadata":{}}],"execution_count":16,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"e4b0be35-e553-4c9b-8d28-1f35145b294d"},{"cell_type":"markdown","source":["### Main Loop - Import all W2 Scans from the 'New_Files' folder\n","Note: this synchronous process is for demonstration/PoC purposes.  In a production solution, effort should be made to separate submission to Azure AI and processing or Azure AI results as batch processes."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c668fcaa-5c79-4d48-b130-aa72d9dbcd72"},{"cell_type":"code","source":["import os\n","import base64\n","# Step 1 - Read input JPG files\n","df = spark.read.format(\"binaryFile\").load(source_folder)\n","\n","# for each JPG, read data field using Azure AI Services\n","for row in df.rdd.collect():\n","    path = row.path\n","    blob = row.content\n","    filename = os.path.basename(path).split('/')[-1]\n","    print(f\"Processing: {filename}\")\n","\n","    # Encode JPG as Base64 String for submission to Azure AI Service\n","    encoded = base64.b64encode(blob).decode('ascii')\n","    jsonInput = {\n","        \"base64Source\": encoded\n","    }\n","\n","    # Call Azure AI to extract data from .jpg file, return as Spark DataFrame\n","    output = analyze_tax_us_w2(filename, jsonInput)\n","\n","    # Append data to data frame and write to Lakehouse Table\n","    save_batch_to_table(output)\n","\n","    # save completed file to archive folder\n","    if not os.path.exists(archive_folder):\n","        os.makedirs(archive_folder)\n","\n","    archive_file_path = os.path.join(archive_folder, filename)\n","    archive_file = open(archive_file_path,\"wb\") \n","    archive_file.write(blob) \n","    archive_file.close()\n","\n","    # Delete original file from source_folder\n","    new_file_path = os.path.join(source_folder_file_api, filename)\n","    os.remove(new_file_path)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"35f9dffe-c7ce-4050-a068-dd8e1c3ab94e","statement_id":29,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-06T18:09:19.5289826Z","session_start_time":null,"execution_start_time":"2024-01-06T18:09:19.8309062Z","execution_finish_time":"2024-01-06T18:14:32.6562782Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":44},"jobs":[{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":0,"dataRead":4919,"rowCount":50,"usageDescription":"","jobId":52,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 29:\nimport os\nimport base64\n# Step 1 - Read input JPG files\ndf = spark.read.format(\"binaryFile\").load(source_folder)\n\n# for each JPG, read data field using Azure AI Services\nfor row in df.rdd.collect():\n    path = row.path\n    blob = row.content\n    filename = os.path.basename(path).split('/')[-1]\n    print(f\"Processing: {filename}\")\n\n    # Encode JPG as Base64 String for submission to Azure AI Service\n    encoded = base64.b64encode(blob).decode('ascii')\n    jsonInput = {\n        \"base64Source\": encoded\n    }\n\n    # Call Azure AI to extract data from .jpg file, return as Spark DataFrame\n    output = analyze_tax_us_w2(filename, jsonInput)\n\n    # Append data to data frame and write to Lakehouse Table\n    save_batch_to_table(output)\n\n    # save completed file to archive folder\n    if not os.path.exists(archive_folder):\n        os.makedirs(archive_folder)\n\n    archive_file_path = os.path.join(archive_folder, filename)\n    archive_file = open(archive_file_path,\"wb\") \n    archive_file.write(blob) \n    archive_file.clos...: Compute snapshot for version: 17","submissionTime":"2024-01-06T18:14:31.231GMT","completionTime":"2024-01-06T18:14:31.269GMT","stageIds":[84,82,83],"jobGroup":"29","status":"SUCCEEDED","numTasks":59,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":58,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":4919,"dataRead":27758,"rowCount":77,"usageDescription":"","jobId":51,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 29:\nimport os\nimport base64\n# Step 1 - Read input JPG files\ndf = spark.read.format(\"binaryFile\").load(source_folder)\n\n# for each JPG, read data field using Azure AI Services\nfor row in df.rdd.collect():\n    path = row.path\n    blob = row.content\n    filename = os.path.basename(path).split('/')[-1]\n    print(f\"Processing: {filename}\")\n\n    # Encode JPG as Base64 String for submission to Azure AI Service\n    encoded = base64.b64encode(blob).decode('ascii')\n    jsonInput = {\n        \"base64Source\": encoded\n    }\n\n    # Call Azure AI to extract data from .jpg file, return as Spark DataFrame\n    output = analyze_tax_us_w2(filename, jsonInput)\n\n    # Append data to data frame and write to Lakehouse Table\n    save_batch_to_table(output)\n\n    # save completed file to archive folder\n    if not os.path.exists(archive_folder):\n        os.makedirs(archive_folder)\n\n    archive_file_path = os.path.join(archive_folder, filename)\n    archive_file = open(archive_file_path,\"wb\") \n    archive_file.write(blob) \n    archive_file.clos...: Compute snapshot for version: 17","submissionTime":"2024-01-06T18:14:30.682GMT","completionTime":"2024-01-06T18:14:31.209GMT","stageIds":[81,80],"jobGroup":"29","status":"SUCCEEDED","numTasks":58,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":27758,"dataRead":22755,"rowCount":54,"usageDescription":"","jobId":50,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 29:\nimport os\nimport base64\n# Step 1 - Read input JPG files\ndf = spark.read.format(\"binaryFile\").load(source_folder)\n\n# for each JPG, read data field using Azure AI Services\nfor row in df.rdd.collect():\n    path = row.path\n    blob = row.content\n    filename = os.path.basename(path).split('/')[-1]\n    print(f\"Processing: {filename}\")\n\n    # Encode JPG as Base64 String for submission to Azure AI Service\n    encoded = base64.b64encode(blob).decode('ascii')\n    jsonInput = {\n        \"base64Source\": encoded\n    }\n\n    # Call Azure AI to extract data from .jpg file, return as Spark DataFrame\n    output = analyze_tax_us_w2(filename, jsonInput)\n\n    # Append data to data frame and write to Lakehouse Table\n    save_batch_to_table(output)\n\n    # save completed file to archive folder\n    if not os.path.exists(archive_folder):\n        os.makedirs(archive_folder)\n\n    archive_file_path = os.path.join(archive_folder, filename)\n    archive_file = open(archive_file_path,\"wb\") \n    archive_file.write(blob) \n    archive_file.clos...: Compute snapshot for version: 17","submissionTime":"2024-01-06T18:14:30.441GMT","completionTime":"2024-01-06T18:14:30.552GMT","stageIds":[79],"jobGroup":"29","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2951","dataWritten":0,"dataRead":22755,"rowCount":27,"usageDescription":"","jobId":49,"name":"toString at String.java:2951","description":"Job group for statement 29:\nimport os\nimport base64\n# Step 1 - Read input JPG files\ndf = spark.read.format(\"binaryFile\").load(source_folder)\n\n# for each JPG, read data field using Azure AI Services\nfor row in df.rdd.collect():\n    path = row.path\n    blob = row.content\n    filename = os.path.basename(path).split('/')[-1]\n    print(f\"Processing: {filename}\")\n\n    # Encode JPG as Base64 String for submission to Azure AI Service\n    encoded = base64.b64encode(blob).decode('ascii')\n    jsonInput = {\n        \"base64Source\": encoded\n    }\n\n    # Call Azure AI to extract data from .jpg file, return as Spark DataFrame\n    output = analyze_tax_us_w2(filename, jsonInput)\n\n    # Append data to data frame and write to Lakehouse Table\n    save_batch_to_table(output)\n\n    # save completed file to archive folder\n    if not os.path.exists(archive_folder):\n        os.makedirs(archive_folder)\n\n    archive_file_path = os.path.join(archive_folder, filename)\n    archive_file = open(archive_file_path,\"wb\") \n    archive_file.write(blob) \n    archive_file.clos...","submissionTime":"2024-01-06T18:14:30.114GMT","completionTime":"2024-01-06T18:14:30.207GMT","stageIds":[78],"jobGroup":"29","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"saveAsTable at NativeMethodAccessorImpl.java:0","dataWritten":15588,"dataRead":530,"rowCount":2,"usageDescription":"","jobId":48,"name":"saveAsTable at NativeMethodAccessorImpl.java:0","description":"Job group for statement 29:\nimport os\nimport base64\n# Step 1 - Read input JPG files\ndf = spark.read.format(\"binaryFile\").load(source_folder)\n\n# for each JPG, read data field using Azure AI Services\nfor row in df.rdd.collect():\n    path = row.path\n    blob = row.content\n    filename = os.path.basename(path).split('/')[-1]\n    print(f\"Processing: {filename}\")\n\n    # Encode JPG as Base64 String for submission to Azure AI Service\n    encoded = base64.b64encode(blob).decode('ascii')\n    jsonInput = {\n        \"base64Source\": encoded\n    }\n\n    # Call Azure AI to extract data from .jpg file, return as Spark DataFrame\n    output = analyze_tax_us_w2(filename, jsonInput)\n\n    # Append data to data frame and write to Lakehouse Table\n    save_batch_to_table(output)\n\n    # save completed file to archive folder\n    if not os.path.exists(archive_folder):\n        os.makedirs(archive_folder)\n\n    archive_file_path = os.path.join(archive_folder, filename)\n    archive_file = open(archive_file_path,\"wb\") \n    archive_file.write(blob) \n    archive_file.clos...","submissionTime":"2024-01-06T18:14:21.101GMT","completionTime":"2024-01-06T18:14:28.364GMT","stageIds":[76,77],"jobGroup":"29","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"saveAsTable at NativeMethodAccessorImpl.java:0","dataWritten":530,"dataRead":0,"rowCount":1,"usageDescription":"","jobId":47,"name":"saveAsTable at NativeMethodAccessorImpl.java:0","description":"Job group for statement 29:\nimport os\nimport base64\n# Step 1 - Read input JPG files\ndf = spark.read.format(\"binaryFile\").load(source_folder)\n\n# for each JPG, read data field using Azure AI Services\nfor row in df.rdd.collect():\n    path = row.path\n    blob = row.content\n    filename = os.path.basename(path).split('/')[-1]\n    print(f\"Processing: {filename}\")\n\n    # Encode JPG as Base64 String for submission to Azure AI Service\n    encoded = base64.b64encode(blob).decode('ascii')\n    jsonInput = {\n        \"base64Source\": encoded\n    }\n\n    # Call Azure AI to extract data from .jpg file, return as Spark DataFrame\n    output = analyze_tax_us_w2(filename, jsonInput)\n\n    # Append data to data frame and write to Lakehouse Table\n    save_batch_to_table(output)\n\n    # save completed file to archive folder\n    if not os.path.exists(archive_folder):\n        os.makedirs(archive_folder)\n\n    archive_file_path = os.path.join(archive_folder, filename)\n    archive_file = open(archive_file_path,\"wb\") \n    archive_file.write(blob) \n    archive_file.clos...","submissionTime":"2024-01-06T18:14:20.856GMT","completionTime":"2024-01-06T18:14:21.051GMT","stageIds":[75],"jobGroup":"29","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":0,"dataRead":4906,"rowCount":50,"usageDescription":"","jobId":46,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 29:\nimport os\nimport base64\n# Step 1 - Read input JPG files\ndf = spark.read.format(\"binaryFile\").load(source_folder)\n\n# for each JPG, read data field using Azure AI Services\nfor row in df.rdd.collect():\n    path = row.path\n    blob = row.content\n    filename = os.path.basename(path).split('/')[-1]\n    print(f\"Processing: {filename}\")\n\n    # Encode JPG as Base64 String for submission to Azure AI Service\n    encoded = base64.b64encode(blob).decode('ascii')\n    jsonInput = {\n        \"base64Source\": encoded\n    }\n\n    # Call Azure AI to extract data from .jpg file, return as Spark DataFrame\n    output = analyze_tax_us_w2(filename, jsonInput)\n\n    # Append data to data frame and write to Lakehouse Table\n    save_batch_to_table(output)\n\n    # save completed file to archive folder\n    if not os.path.exists(archive_folder):\n        os.makedirs(archive_folder)\n\n    archive_file_path = os.path.join(archive_folder, filename)\n    archive_file = open(archive_file_path,\"wb\") \n    archive_file.write(blob) \n    archive_file.clos...: Compute snapshot for version: 16","submissionTime":"2024-01-06T18:13:49.246GMT","completionTime":"2024-01-06T18:13:49.288GMT","stageIds":[74,72,73],"jobGroup":"29","status":"SUCCEEDED","numTasks":58,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":57,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":4906,"dataRead":25836,"rowCount":75,"usageDescription":"","jobId":45,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 29:\nimport os\nimport base64\n# Step 1 - Read input JPG files\ndf = spark.read.format(\"binaryFile\").load(source_folder)\n\n# for each JPG, read data field using Azure AI Services\nfor row in df.rdd.collect():\n    path = row.path\n    blob = row.content\n    filename = os.path.basename(path).split('/')[-1]\n    print(f\"Processing: {filename}\")\n\n    # Encode JPG as Base64 String for submission to Azure AI Service\n    encoded = base64.b64encode(blob).decode('ascii')\n    jsonInput = {\n        \"base64Source\": encoded\n    }\n\n    # Call Azure AI to extract data from .jpg file, return as Spark DataFrame\n    output = analyze_tax_us_w2(filename, jsonInput)\n\n    # Append data to data frame and write to Lakehouse Table\n    save_batch_to_table(output)\n\n    # save completed file to archive folder\n    if not os.path.exists(archive_folder):\n        os.makedirs(archive_folder)\n\n    archive_file_path = os.path.join(archive_folder, filename)\n    archive_file = open(archive_file_path,\"wb\") \n    archive_file.write(blob) \n    archive_file.clos...: Compute snapshot for version: 16","submissionTime":"2024-01-06T18:13:48.626GMT","completionTime":"2024-01-06T18:13:49.230GMT","stageIds":[70,71],"jobGroup":"29","status":"SUCCEEDED","numTasks":57,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":7,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":25836,"dataRead":19482,"rowCount":50,"usageDescription":"","jobId":44,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 29:\nimport os\nimport base64\n# Step 1 - Read input JPG files\ndf = spark.read.format(\"binaryFile\").load(source_folder)\n\n# for each JPG, read data field using Azure AI Services\nfor row in df.rdd.collect():\n    path = row.path\n    blob = row.content\n    filename = os.path.basename(path).split('/')[-1]\n    print(f\"Processing: {filename}\")\n\n    # Encode JPG as Base64 String for submission to Azure AI Service\n    encoded = base64.b64encode(blob).decode('ascii')\n    jsonInput = {\n        \"base64Source\": encoded\n    }\n\n    # Call Azure AI to extract data from .jpg file, return as Spark DataFrame\n    output = analyze_tax_us_w2(filename, jsonInput)\n\n    # Append data to data frame and write to Lakehouse Table\n    save_batch_to_table(output)\n\n    # save completed file to archive folder\n    if not os.path.exists(archive_folder):\n        os.makedirs(archive_folder)\n\n    archive_file_path = os.path.join(archive_folder, filename)\n    archive_file = open(archive_file_path,\"wb\") \n    archive_file.write(blob) \n    archive_file.clos...: Compute snapshot for version: 16","submissionTime":"2024-01-06T18:13:48.348GMT","completionTime":"2024-01-06T18:13:48.484GMT","stageIds":[69],"jobGroup":"29","status":"SUCCEEDED","numTasks":7,"numActiveTasks":0,"numCompletedTasks":7,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":7,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2951","dataWritten":0,"dataRead":19482,"rowCount":25,"usageDescription":"","jobId":43,"name":"toString at String.java:2951","description":"Job group for statement 29:\nimport os\nimport base64\n# Step 1 - Read input JPG files\ndf = spark.read.format(\"binaryFile\").load(source_folder)\n\n# for each JPG, read data field using Azure AI Services\nfor row in df.rdd.collect():\n    path = row.path\n    blob = row.content\n    filename = os.path.basename(path).split('/')[-1]\n    print(f\"Processing: {filename}\")\n\n    # Encode JPG as Base64 String for submission to Azure AI Service\n    encoded = base64.b64encode(blob).decode('ascii')\n    jsonInput = {\n        \"base64Source\": encoded\n    }\n\n    # Call Azure AI to extract data from .jpg file, return as Spark DataFrame\n    output = analyze_tax_us_w2(filename, jsonInput)\n\n    # Append data to data frame and write to Lakehouse Table\n    save_batch_to_table(output)\n\n    # save completed file to archive folder\n    if not os.path.exists(archive_folder):\n        os.makedirs(archive_folder)\n\n    archive_file_path = os.path.join(archive_folder, filename)\n    archive_file = open(archive_file_path,\"wb\") \n    archive_file.write(blob) \n    archive_file.clos...","submissionTime":"2024-01-06T18:13:47.994GMT","completionTime":"2024-01-06T18:13:48.120GMT","stageIds":[68],"jobGroup":"29","status":"SUCCEEDED","numTasks":7,"numActiveTasks":0,"numCompletedTasks":7,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":7,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"saveAsTable at NativeMethodAccessorImpl.java:0","dataWritten":15572,"dataRead":518,"rowCount":2,"usageDescription":"","jobId":42,"name":"saveAsTable at NativeMethodAccessorImpl.java:0","description":"Job group for statement 29:\nimport os\nimport base64\n# Step 1 - Read input JPG files\ndf = spark.read.format(\"binaryFile\").load(source_folder)\n\n# for each JPG, read data field using Azure AI Services\nfor row in df.rdd.collect():\n    path = row.path\n    blob = row.content\n    filename = os.path.basename(path).split('/')[-1]\n    print(f\"Processing: {filename}\")\n\n    # Encode JPG as Base64 String for submission to Azure AI Service\n    encoded = base64.b64encode(blob).decode('ascii')\n    jsonInput = {\n        \"base64Source\": encoded\n    }\n\n    # Call Azure AI to extract data from .jpg file, return as Spark DataFrame\n    output = analyze_tax_us_w2(filename, jsonInput)\n\n    # Append data to data frame and write to Lakehouse Table\n    save_batch_to_table(output)\n\n    # save completed file to archive folder\n    if not os.path.exists(archive_folder):\n        os.makedirs(archive_folder)\n\n    archive_file_path = os.path.join(archive_folder, filename)\n    archive_file = open(archive_file_path,\"wb\") \n    archive_file.write(blob) \n    archive_file.clos...","submissionTime":"2024-01-06T18:13:41.570GMT","completionTime":"2024-01-06T18:13:46.654GMT","stageIds":[66,67],"jobGroup":"29","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"saveAsTable at NativeMethodAccessorImpl.java:0","dataWritten":518,"dataRead":0,"rowCount":1,"usageDescription":"","jobId":41,"name":"saveAsTable at NativeMethodAccessorImpl.java:0","description":"Job group for statement 29:\nimport os\nimport base64\n# Step 1 - Read input JPG files\ndf = spark.read.format(\"binaryFile\").load(source_folder)\n\n# for each JPG, read data field using Azure AI Services\nfor row in df.rdd.collect():\n    path = row.path\n    blob = row.content\n    filename = os.path.basename(path).split('/')[-1]\n    print(f\"Processing: {filename}\")\n\n    # Encode JPG as Base64 String for submission to Azure AI Service\n    encoded = base64.b64encode(blob).decode('ascii')\n    jsonInput = {\n        \"base64Source\": encoded\n    }\n\n    # Call Azure AI to extract data from .jpg file, return as Spark DataFrame\n    output = analyze_tax_us_w2(filename, jsonInput)\n\n    # Append data to data frame and write to Lakehouse Table\n    save_batch_to_table(output)\n\n    # save completed file to archive folder\n    if not os.path.exists(archive_folder):\n        os.makedirs(archive_folder)\n\n    archive_file_path = os.path.join(archive_folder, filename)\n    archive_file = open(archive_file_path,\"wb\") \n    archive_file.write(blob) \n    archive_file.clos...","submissionTime":"2024-01-06T18:13:41.338GMT","completionTime":"2024-01-06T18:13:41.521GMT","stageIds":[65],"jobGroup":"29","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":0,"dataRead":4893,"rowCount":50,"usageDescription":"","jobId":40,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 29:\nimport os\nimport base64\n# Step 1 - Read input JPG files\ndf = spark.read.format(\"binaryFile\").load(source_folder)\n\n# for each JPG, read data field using Azure AI Services\nfor row in df.rdd.collect():\n    path = row.path\n    blob = row.content\n    filename = os.path.basename(path).split('/')[-1]\n    print(f\"Processing: {filename}\")\n\n    # Encode JPG as Base64 String for submission to Azure AI Service\n    encoded = base64.b64encode(blob).decode('ascii')\n    jsonInput = {\n        \"base64Source\": encoded\n    }\n\n    # Call Azure AI to extract data from .jpg file, return as Spark DataFrame\n    output = analyze_tax_us_w2(filename, jsonInput)\n\n    # Append data to data frame and write to Lakehouse Table\n    save_batch_to_table(output)\n\n    # save completed file to archive folder\n    if not os.path.exists(archive_folder):\n        os.makedirs(archive_folder)\n\n    archive_file_path = os.path.join(archive_folder, filename)\n    archive_file = open(archive_file_path,\"wb\") \n    archive_file.write(blob) \n    archive_file.clos...: Compute snapshot for version: 15","submissionTime":"2024-01-06T18:13:09.589GMT","completionTime":"2024-01-06T18:13:09.630GMT","stageIds":[63,64,62],"jobGroup":"29","status":"SUCCEEDED","numTasks":57,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":56,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":4893,"dataRead":23909,"rowCount":73,"usageDescription":"","jobId":39,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 29:\nimport os\nimport base64\n# Step 1 - Read input JPG files\ndf = spark.read.format(\"binaryFile\").load(source_folder)\n\n# for each JPG, read data field using Azure AI Services\nfor row in df.rdd.collect():\n    path = row.path\n    blob = row.content\n    filename = os.path.basename(path).split('/')[-1]\n    print(f\"Processing: {filename}\")\n\n    # Encode JPG as Base64 String for submission to Azure AI Service\n    encoded = base64.b64encode(blob).decode('ascii')\n    jsonInput = {\n        \"base64Source\": encoded\n    }\n\n    # Call Azure AI to extract data from .jpg file, return as Spark DataFrame\n    output = analyze_tax_us_w2(filename, jsonInput)\n\n    # Append data to data frame and write to Lakehouse Table\n    save_batch_to_table(output)\n\n    # save completed file to archive folder\n    if not os.path.exists(archive_folder):\n        os.makedirs(archive_folder)\n\n    archive_file_path = os.path.join(archive_folder, filename)\n    archive_file = open(archive_file_path,\"wb\") \n    archive_file.write(blob) \n    archive_file.clos...: Compute snapshot for version: 15","submissionTime":"2024-01-06T18:13:08.922GMT","completionTime":"2024-01-06T18:13:09.566GMT","stageIds":[60,61],"jobGroup":"29","status":"SUCCEEDED","numTasks":56,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":6,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":23909,"dataRead":16284,"rowCount":46,"usageDescription":"","jobId":38,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 29:\nimport os\nimport base64\n# Step 1 - Read input JPG files\ndf = spark.read.format(\"binaryFile\").load(source_folder)\n\n# for each JPG, read data field using Azure AI Services\nfor row in df.rdd.collect():\n    path = row.path\n    blob = row.content\n    filename = os.path.basename(path).split('/')[-1]\n    print(f\"Processing: {filename}\")\n\n    # Encode JPG as Base64 String for submission to Azure AI Service\n    encoded = base64.b64encode(blob).decode('ascii')\n    jsonInput = {\n        \"base64Source\": encoded\n    }\n\n    # Call Azure AI to extract data from .jpg file, return as Spark DataFrame\n    output = analyze_tax_us_w2(filename, jsonInput)\n\n    # Append data to data frame and write to Lakehouse Table\n    save_batch_to_table(output)\n\n    # save completed file to archive folder\n    if not os.path.exists(archive_folder):\n        os.makedirs(archive_folder)\n\n    archive_file_path = os.path.join(archive_folder, filename)\n    archive_file = open(archive_file_path,\"wb\") \n    archive_file.write(blob) \n    archive_file.clos...: Compute snapshot for version: 15","submissionTime":"2024-01-06T18:13:08.657GMT","completionTime":"2024-01-06T18:13:08.757GMT","stageIds":[59],"jobGroup":"29","status":"SUCCEEDED","numTasks":6,"numActiveTasks":0,"numCompletedTasks":6,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":6,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2951","dataWritten":0,"dataRead":16284,"rowCount":23,"usageDescription":"","jobId":37,"name":"toString at String.java:2951","description":"Job group for statement 29:\nimport os\nimport base64\n# Step 1 - Read input JPG files\ndf = spark.read.format(\"binaryFile\").load(source_folder)\n\n# for each JPG, read data field using Azure AI Services\nfor row in df.rdd.collect():\n    path = row.path\n    blob = row.content\n    filename = os.path.basename(path).split('/')[-1]\n    print(f\"Processing: {filename}\")\n\n    # Encode JPG as Base64 String for submission to Azure AI Service\n    encoded = base64.b64encode(blob).decode('ascii')\n    jsonInput = {\n        \"base64Source\": encoded\n    }\n\n    # Call Azure AI to extract data from .jpg file, return as Spark DataFrame\n    output = analyze_tax_us_w2(filename, jsonInput)\n\n    # Append data to data frame and write to Lakehouse Table\n    save_batch_to_table(output)\n\n    # save completed file to archive folder\n    if not os.path.exists(archive_folder):\n        os.makedirs(archive_folder)\n\n    archive_file_path = os.path.join(archive_folder, filename)\n    archive_file = open(archive_file_path,\"wb\") \n    archive_file.write(blob) \n    archive_file.clos...","submissionTime":"2024-01-06T18:13:08.301GMT","completionTime":"2024-01-06T18:13:08.382GMT","stageIds":[58],"jobGroup":"29","status":"SUCCEEDED","numTasks":6,"numActiveTasks":0,"numCompletedTasks":6,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":6,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"saveAsTable at NativeMethodAccessorImpl.java:0","dataWritten":15574,"dataRead":538,"rowCount":2,"usageDescription":"","jobId":36,"name":"saveAsTable at NativeMethodAccessorImpl.java:0","description":"Job group for statement 29:\nimport os\nimport base64\n# Step 1 - Read input JPG files\ndf = spark.read.format(\"binaryFile\").load(source_folder)\n\n# for each JPG, read data field using Azure AI Services\nfor row in df.rdd.collect():\n    path = row.path\n    blob = row.content\n    filename = os.path.basename(path).split('/')[-1]\n    print(f\"Processing: {filename}\")\n\n    # Encode JPG as Base64 String for submission to Azure AI Service\n    encoded = base64.b64encode(blob).decode('ascii')\n    jsonInput = {\n        \"base64Source\": encoded\n    }\n\n    # Call Azure AI to extract data from .jpg file, return as Spark DataFrame\n    output = analyze_tax_us_w2(filename, jsonInput)\n\n    # Append data to data frame and write to Lakehouse Table\n    save_batch_to_table(output)\n\n    # save completed file to archive folder\n    if not os.path.exists(archive_folder):\n        os.makedirs(archive_folder)\n\n    archive_file_path = os.path.join(archive_folder, filename)\n    archive_file = open(archive_file_path,\"wb\") \n    archive_file.write(blob) \n    archive_file.clos...","submissionTime":"2024-01-06T18:12:36.494GMT","completionTime":"2024-01-06T18:13:06.910GMT","stageIds":[56,57],"jobGroup":"29","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"saveAsTable at NativeMethodAccessorImpl.java:0","dataWritten":538,"dataRead":0,"rowCount":1,"usageDescription":"","jobId":35,"name":"saveAsTable at NativeMethodAccessorImpl.java:0","description":"Job group for statement 29:\nimport os\nimport base64\n# Step 1 - Read input JPG files\ndf = spark.read.format(\"binaryFile\").load(source_folder)\n\n# for each JPG, read data field using Azure AI Services\nfor row in df.rdd.collect():\n    path = row.path\n    blob = row.content\n    filename = os.path.basename(path).split('/')[-1]\n    print(f\"Processing: {filename}\")\n\n    # Encode JPG as Base64 String for submission to Azure AI Service\n    encoded = base64.b64encode(blob).decode('ascii')\n    jsonInput = {\n        \"base64Source\": encoded\n    }\n\n    # Call Azure AI to extract data from .jpg file, return as Spark DataFrame\n    output = analyze_tax_us_w2(filename, jsonInput)\n\n    # Append data to data frame and write to Lakehouse Table\n    save_batch_to_table(output)\n\n    # save completed file to archive folder\n    if not os.path.exists(archive_folder):\n        os.makedirs(archive_folder)\n\n    archive_file_path = os.path.join(archive_folder, filename)\n    archive_file = open(archive_file_path,\"wb\") \n    archive_file.write(blob) \n    archive_file.clos...","submissionTime":"2024-01-06T18:12:36.244GMT","completionTime":"2024-01-06T18:12:36.434GMT","stageIds":[55],"jobGroup":"29","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":0,"dataRead":4880,"rowCount":50,"usageDescription":"","jobId":34,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 29:\nimport os\nimport base64\n# Step 1 - Read input JPG files\ndf = spark.read.format(\"binaryFile\").load(source_folder)\n\n# for each JPG, read data field using Azure AI Services\nfor row in df.rdd.collect():\n    path = row.path\n    blob = row.content\n    filename = os.path.basename(path).split('/')[-1]\n    print(f\"Processing: {filename}\")\n\n    # Encode JPG as Base64 String for submission to Azure AI Service\n    encoded = base64.b64encode(blob).decode('ascii')\n    jsonInput = {\n        \"base64Source\": encoded\n    }\n\n    # Call Azure AI to extract data from .jpg file, return as Spark DataFrame\n    output = analyze_tax_us_w2(filename, jsonInput)\n\n    # Append data to data frame and write to Lakehouse Table\n    save_batch_to_table(output)\n\n    # save completed file to archive folder\n    if not os.path.exists(archive_folder):\n        os.makedirs(archive_folder)\n\n    archive_file_path = os.path.join(archive_folder, filename)\n    archive_file = open(archive_file_path,\"wb\") \n    archive_file.write(blob) \n    archive_file.clos...: Compute snapshot for version: 14","submissionTime":"2024-01-06T18:12:04.677GMT","completionTime":"2024-01-06T18:12:04.785GMT","stageIds":[52,53,54],"jobGroup":"29","status":"SUCCEEDED","numTasks":56,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":55,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":4880,"dataRead":21984,"rowCount":71,"usageDescription":"","jobId":33,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 29:\nimport os\nimport base64\n# Step 1 - Read input JPG files\ndf = spark.read.format(\"binaryFile\").load(source_folder)\n\n# for each JPG, read data field using Azure AI Services\nfor row in df.rdd.collect():\n    path = row.path\n    blob = row.content\n    filename = os.path.basename(path).split('/')[-1]\n    print(f\"Processing: {filename}\")\n\n    # Encode JPG as Base64 String for submission to Azure AI Service\n    encoded = base64.b64encode(blob).decode('ascii')\n    jsonInput = {\n        \"base64Source\": encoded\n    }\n\n    # Call Azure AI to extract data from .jpg file, return as Spark DataFrame\n    output = analyze_tax_us_w2(filename, jsonInput)\n\n    # Append data to data frame and write to Lakehouse Table\n    save_batch_to_table(output)\n\n    # save completed file to archive folder\n    if not os.path.exists(archive_folder):\n        os.makedirs(archive_folder)\n\n    archive_file_path = os.path.join(archive_folder, filename)\n    archive_file = open(archive_file_path,\"wb\") \n    archive_file.write(blob) \n    archive_file.clos...: Compute snapshot for version: 14","submissionTime":"2024-01-06T18:12:04.048GMT","completionTime":"2024-01-06T18:12:04.639GMT","stageIds":[51,50],"jobGroup":"29","status":"SUCCEEDED","numTasks":55,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":5,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"732c6c28-7c9d-49cd-8112-3a95143366f4"},"text/plain":"StatementMeta(, 35f9dffe-c7ce-4050-a068-dd8e1c3ab94e, 29, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Processing: W2_XL_input_clean_1017.jpg\nPolling for Azure AI response...\n...have AI response!\nWriting 1 rows to Delta table.\nProcessing: W2_XL_input_clean_1015.jpg\nPolling for Azure AI response...\n...have AI response!\nWriting 1 rows to Delta table.\nProcessing: W2_XL_input_clean_1013.jpg\nPolling for Azure AI response...\n...have AI response!\nWriting 1 rows to Delta table.\nProcessing: W2_XL_input_clean_1016.jpg\nPolling for Azure AI response...\n...have AI response!\nWriting 1 rows to Delta table.\nProcessing: W2_XL_input_clean_1014.jpg\nPolling for Azure AI response...\n...have AI response!\nWriting 1 rows to Delta table.\nProcessing: W2_XL_input_clean_1018.jpg\nPolling for Azure AI response...\n...have AI response!\nWriting 1 rows to Delta table.\nProcessing: W2_XL_input_clean_1012.jpg\nPolling for Azure AI response...\n...have AI response!\nWriting 1 rows to Delta table.\n"]}],"execution_count":17,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"4994ecf8-5a18-4b9d-a08e-71bf90569f54"},{"cell_type":"code","source":["df = spark.sql(\"SELECT * FROM AI_Demo_LH.forms_w2 order by scanned_filename LIMIT 1000\")\n","display(df)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"35f9dffe-c7ce-4050-a068-dd8e1c3ab94e","statement_id":31,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-06T18:15:21.4151707Z","session_start_time":null,"execution_start_time":"2024-01-06T18:15:21.8130885Z","execution_finish_time":"2024-01-06T18:15:24.2905194Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":2},"jobs":[{"displayName":"getRowsInJsonString at Display.scala:452","dataWritten":0,"dataRead":0,"rowCount":18,"usageDescription":"","jobId":58,"name":"getRowsInJsonString at Display.scala:452","description":"Job group for statement 31:\ndf = spark.sql(\"SELECT * FROM AI_Demo_LH.forms_w2 order by scanned_filename LIMIT 1000\")\ndisplay(df)","submissionTime":"2024-01-06T18:15:23.086GMT","completionTime":"2024-01-06T18:15:23.287GMT","stageIds":[92],"jobGroup":"31","status":"SUCCEEDED","numTasks":6,"numActiveTasks":0,"numCompletedTasks":6,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":6,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":0,"dataRead":19775,"rowCount":20,"usageDescription":"","jobId":57,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 31:\ndf = spark.sql(\"SELECT * FROM AI_Demo_LH.forms_w2 order by scanned_filename LIMIT 1000\")\ndisplay(df): Filtering files for query","submissionTime":"2024-01-06T18:15:22.770GMT","completionTime":"2024-01-06T18:15:22.980GMT","stageIds":[90,91],"jobGroup":"31","status":"SUCCEEDED","numTasks":58,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"688a597e-8090-4d65-8ffc-3e1bb040a1c6"},"text/plain":"StatementMeta(, 35f9dffe-c7ce-4050-a068-dd8e1c3ab94e, 31, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"da371f00-c317-4349-8138-67de9f01d53c","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, da371f00-c317-4349-8138-67de9f01d53c)"},"metadata":{}}],"execution_count":19,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"dec3c0a5-8a7e-4d73-a4d7-44e812913abc"}],"metadata":{"language_info":{"name":"python"},"microsoft":{"host":{"synapse_widget":{"token":"e7e99d65-92d1-4953-a47a-3e37148fcc2f","state":{"da371f00-c317-4349-8138-67de9f01d53c":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"W2_XL_input_clean_1001.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"9744017","5":"Taylor Cox","6":"779-70-5706","7":"613 Roger Crest","8":"Leeton","9":"IA","10":"26442-8249","11":"Carlson Group Group","12":"39-3114215","13":"56319 Underwood Views Lake Debramouth NE","14":"NULL","15":"NULL","16":"33667-9751","17":"126204.09","18":"40043.57","19":"123655.86","20":"9459.67","21":"152406.79","22":"4419.80","23":"123655.86","24":"152406.79","25":"233.00","26":"219.00","27":"false","28":"true","29":"false","index":1},{"0":"W2_XL_input_clean_1002.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"9176733","5":"Laura Russo","6":"675-52-6498","7":"2199 Little Falls","8":"Snyderton","9":"TX","10":"28420-7250","11":"Marshall PLC PLC","12":"73-1267148","13":"6822 Henry Neck","14":"Port Angelastad","15":"IA","16":"13833-3941","17":"86286.94","18":"23792.16","19":"90335.60","20":"6910.67","21":"67631.41","22":"1961.31","23":"90335.60","24":"67631.41","25":"259.00","26":"267.00","27":"false","28":"true","29":"false","index":2},{"0":"W2_XL_input_clean_1003.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"2008341","5":"Mitchell Allen","6":"691-53-3104","7":"6503 John Stream New Meredithstad MI 19628-8","8":"NULL","9":"NULL","10":"NULL","11":"Mcdonald-Harris Inc","12":"25-0924379","13":"560 Munoz Mills Suite 766 South","14":"Jamesstad","15":"MN","16":"44881-3192","17":"238401.80","18":"32110.03","19":"295439.81","20":"22601.15","21":"299378.97","22":"8681.99","23":"295439.81","24":"299378.97","25":"162.00","26":"260.00","27":"false","28":"true","29":"false","index":3},{"0":"W2_XL_input_clean_1004.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"343207","5":"James Williams","6":"037-07-1416","7":"95060 Crystal Burg","8":"Davisburgh","9":"AR","10":"30937-0798","11":"Frederick-Hendricks Group","12":"71-6374971","13":"01670 Adams Islands","14":"Angelatown","15":"AZ","16":"21706-7217","17":"249501.21","18":"26149.11","19":"254242.78","20":"19449.57","21":"224061.76","22":"6497.79","23":"254242.78","24":"224061.76","25":"164.00","26":"150.00","27":"true","28":"false","29":"false","index":4},{"0":"W2_XL_input_clean_1005.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"7480956","5":"Trevor Duncan","6":"768-05-9800","7":"41435 Hughes Drive","8":"New Teresaberg","9":"KS","10":"99528-9775","11":"White PLC Inc","12":"83-1015714","13":"None None","14":"Walker Landing Bakerfurt","15":"NJ","16":"21769","17":"168250.20","18":"53860.77","19":"159372.62","20":"12192.01","21":"206499.26","22":"5988.48","23":"159372.62","24":"206499.26","25":"149.00","26":"164.00","27":"false","28":"false","29":"false","index":5},{"0":"W2_XL_input_clean_1006.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"6307582","5":"Mary Dickerson","6":"036-91-4705","7":"95842 Freeman Coves","8":"Robertburgh","9":"MA","10":"63545-3314","11":"Morgan Ltd and Sons","12":"22-3437795","13":"147 Taylor Wall","14":"NULL","15":"NULL","16":"NULL","17":"237575.44","18":"85837.08","19":"278526.87","20":"21307.31","21":"182944.39","22":"5305.39","23":"278526.87","24":"182944.39","25":"123.00","26":"171.00","27":"false","28":"false","29":"false","index":6},{"0":"W2_XL_input_clean_1007.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"396234","5":"Seth Bailey","6":"760-19-8138","7":"341 Charles Mountains Morganberg","8":"NULL","9":"co","10":"47155-8501","11":"Brown LLC Group","12":"21-7657558","13":"None None","14":"NULL","15":"NULL","16":"NULL","17":"162805.19","18":"57402.86","19":"184235.06","20":"14093.98","21":"121322.24","22":"3518.34","23":"184235.06","24":"121322.24","25":"148.00","26":"119.00","27":"false","28":"true","29":"false","index":7},{"0":"W2_XL_input_clean_1008.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"4858295","5":"Robert Benson","6":"280-96-5377","7":"84813 Eric Way","8":"NULL","9":"NULL","10":"NULL","11":"Flores, Hubbard and Franklin Inc","12":"33-0179104","13":"638 Jennifer Springs","14":"Adamsmouth","15":"OR","16":"71848-7881","17":"71292.15","18":"22405.22","19":"59800.89","20":"4574.77","21":"65211.43","22":"1891.13","23":"59800.89","24":"65211.43","25":"300.00","26":"287.00","27":"false","28":"false","29":"true","index":8},{"0":"W2_XL_input_clean_1009.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"4458678","5":"Michelle Brown","6":"133-45-6693","7":"730 Huber Island","8":"Collinshaven","9":"IN","10":"29316-8972","11":"Cooper, Spence and Curry Ltd","12":"65-8736227","13":"871 Tonya Causeway New Davidborough AL 63538-3453","14":"NULL","15":"NULL","16":"NULL","17":"165296.64","18":"49693.12","19":"198300.45","20":"15169.98","21":"164615.68","22":"4773.85","23":"198300.45","24":"164615.68","25":"148.00","26":"245.00","27":"false","28":"true","29":"false","index":9},{"0":"W2_XL_input_clean_1010.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"3306669","5":"Bobby Torres","6":"592-52-8367","7":"9622 Jessica Estates","8":"Nathantown","9":"IA","10":"78428-1174","11":"Schaefer Group Ltd","12":"36-2682044","13":"18648 Costa Valleys","14":"78136-7800","15":"MI","16":"NULL","17":"52229.12","18":"7055.12","19":"65099.68","20":"4980.13","21":"62139.78","22":"1802.05","23":"65099.68","24":"62139.78","25":"195.00","26":"254.00","27":"true","28":"false","29":"false","index":10},{"0":"W2_XL_input_clean_1011.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"6208470","5":"Jill Case","6":"259-79-5957","7":"08470 Miller Stream","8":"Petersmouth","9":"IN","10":"55359-2536","11":"Young-Perkins PLC","12":"68-0385441","13":"03348 Parker Hills Garciaside NE","14":"NULL","15":"NULL","16":"06534-6570","17":"231779.84","18":"39340.33","19":"238491.85","20":"18244.63","21":"189412.80","22":"5492.97","23":"238491.85","24":"189412.80","25":"158.00","26":"223.00","27":"false","28":"false","29":"true","index":11},{"0":"W2_XL_input_clean_1012.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"3674217","5":"Joseph Miller","6":"246-23-8883","7":"68390 Blackwell Drive South","8":"Robert","9":"MD","10":"20384-0111","11":"Mitchell-Price Ltd","12":"91-4181453","13":"5154 Amanda Station","14":"Jessicastad","15":"MA","16":"10423-1803","17":"50379.86","18":"9384.61","19":"57145.11","20":"4371.60","21":"63120.83","22":"1830.50","23":"57145.11","24":"63120.83","25":"101.00","26":"110.00","27":"true","28":"false","29":"false","index":12},{"0":"W2_XL_input_clean_1013.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"3827600","5":"Jose Booker","6":"725-40-6045","7":"27374 Henderson Oval","8":"Lake James","9":"NJ","10":"32949-6377","11":"Mendoza-Matthews Group","12":"75-1618704","13":"712 Haley Meadow West","14":"Jeremymouth","15":"MD","16":"23850-8575","17":"242353.95","18":"66000.12","19":"191154.72","20":"14623.34","21":"200054.81","22":"5801.59","23":"191154.72","24":"200054.81","25":"216.00","26":"214.00","27":"false","28":"false","29":"true","index":13},{"0":"W2_XL_input_clean_1014.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"6605302","5":"Virginia Smith","6":"107-41-1388","7":"917 April Isle New","8":"Taylor","9":"IL","10":"20087-0048","11":"Bailey-Gross and Sons","12":"25-4904377","13":"2965 James Manors","14":"Christopherburgh","15":"OK","16":"23062-2861","17":"106876.37","18":"16645.06","19":"95686.59","20":"7320.02","21":"93749.00","22":"2718.72","23":"95686.59","24":"93749.00","25":"260.00","26":"248.00","27":"true","28":"false","29":"false","index":14},{"0":"W2_XL_input_clean_1015.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"2302462","5":"Aaron Swanson","6":"466-91-3126","7":"672 Brian Common Suite 123 East Lisa UT","8":"NULL","9":"NULL","10":"55739-1243","11":"Douglas LLC Ltd","12":"63-3760542","13":"84831 Hill Centers","14":"Port Christianberg","15":"KS","16":"62700-6312","17":"94560.78","18":"32659.11","19":"68248.81","20":"5221.03","21":"84120.42","22":"2439.49","23":"68248.81","24":"84120.42","25":"174.00","26":"281.00","27":"true","28":"false","29":"false","index":15},{"0":"W2_XL_input_clean_1016.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"5614245","5":"Laura Daniel","6":"051-22-2256","7":"73200 Tammy Bridge","8":"Chadfort","9":"ND","10":"32994-7451","11":"Jones-Jones Group","12":"42-1580203","13":"706 Cook Island","14":"Rubiotown","15":"IN","16":"25520-9318","17":"113671.47","18":"28250.59","19":"119797.41","20":"9164.50","21":"123417.38","22":"3579.10","23":"119797.41","24":"123417.38","25":"213.00","26":"234.00","27":"false","28":"false","29":"false","index":16},{"0":"W2_XL_input_clean_1017.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"4465330","5":"Kristi Johnson","6":"809-28-4999","7":"83396 Cross Pines","8":"Johnstonville","9":"SC","10":"94521-0987","11":"Thomas-Boyd Inc","12":"21-7956016","13":"0432 Taylor Square South","14":"Georgeton","15":"IN","16":"58391-0590","17":"232115.87","18":"24902.68","19":"163848.60","20":"12534.42","21":"299901.92","22":"8697.16","23":"163848.60","24":"299901.92","25":"279.00","26":"244.00","27":"true","28":"true","29":"false","index":17},{"0":"W2_XL_input_clean_1018.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"7029938","5":"Amy Cline","6":"536-93-9326","7":"8632 Mcclure Curve","8":"Gailfort","9":"NJ","10":"47117-3789","11":"Rangel, Wiggins and Ortega Ltd","12":"82-4064182","13":"3460 Stevenson Islands Sarahbury WY","14":"NULL","15":"NULL","16":"01038-0096","17":"157070.83","18":"37579.09","19":"135198.88","20":"10342.71","21":"153972.01","22":"4465.19","23":"135198.88","24":"153972.01","25":"276.00","26":"280.00","27":"false","28":"false","29":"false","index":18}],"schema":[{"key":"0","name":"scanned_filename","type":"string"},{"key":"1","name":"form_variant","type":"string"},{"key":"2","name":"tax_year","type":"string"},{"key":"3","name":"w2_copy","type":"string"},{"key":"4","name":"control_number","type":"string"},{"key":"5","name":"employee_name","type":"string"},{"key":"6","name":"employee_ssn","type":"string"},{"key":"7","name":"employee_street","type":"string"},{"key":"8","name":"employee_city","type":"string"},{"key":"9","name":"employee_state","type":"string"},{"key":"10","name":"employee_postal_code","type":"string"},{"key":"11","name":"employer_name","type":"string"},{"key":"12","name":"employer_id","type":"string"},{"key":"13","name":"employer_street","type":"string"},{"key":"14","name":"employer_city","type":"string"},{"key":"15","name":"employer_state","type":"string"},{"key":"16","name":"employer_postal_code","type":"string"},{"key":"17","name":"wages_tips","type":"decimal"},{"key":"18","name":"fed_income_tax_withheld","type":"decimal"},{"key":"19","name":"social_security_wages","type":"decimal"},{"key":"20","name":"social_security_tax_withheld","type":"decimal"},{"key":"21","name":"medicare_wages_tips","type":"decimal"},{"key":"22","name":"medicare_tax_withheld","type":"decimal"},{"key":"23","name":"social_security_tips","type":"decimal"},{"key":"24","name":"allocated_tips","type":"decimal"},{"key":"25","name":"non_qualified_plans","type":"decimal"},{"key":"26","name":"dependent_care_benefits","type":"decimal"},{"key":"27","name":"is_statutory_employee","type":"boolean"},{"key":"28","name":"is_retirement_plan","type":"boolean"},{"key":"29","name":"is_third_party_sick_pay","type":"boolean"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["17"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}}}}},"language":"python"},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{"da371f00-c317-4349-8138-67de9f01d53c":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"W2_XL_input_clean_1001.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"9744017","5":"Taylor Cox","6":"779-70-5706","7":"613 Roger Crest","8":"Leeton","9":"IA","10":"26442-8249","11":"Carlson Group Group","12":"39-3114215","13":"56319 Underwood Views Lake Debramouth NE","14":"NULL","15":"NULL","16":"33667-9751","17":"126204.09","18":"40043.57","19":"123655.86","20":"9459.67","21":"152406.79","22":"4419.80","23":"123655.86","24":"152406.79","25":"233.00","26":"219.00","27":"false","28":"true","29":"false","index":1},{"0":"W2_XL_input_clean_1002.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"9176733","5":"Laura Russo","6":"675-52-6498","7":"2199 Little Falls","8":"Snyderton","9":"TX","10":"28420-7250","11":"Marshall PLC PLC","12":"73-1267148","13":"6822 Henry Neck","14":"Port Angelastad","15":"IA","16":"13833-3941","17":"86286.94","18":"23792.16","19":"90335.60","20":"6910.67","21":"67631.41","22":"1961.31","23":"90335.60","24":"67631.41","25":"259.00","26":"267.00","27":"false","28":"true","29":"false","index":2},{"0":"W2_XL_input_clean_1003.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"2008341","5":"Mitchell Allen","6":"691-53-3104","7":"6503 John Stream New Meredithstad MI 19628-8","8":"NULL","9":"NULL","10":"NULL","11":"Mcdonald-Harris Inc","12":"25-0924379","13":"560 Munoz Mills Suite 766 South","14":"Jamesstad","15":"MN","16":"44881-3192","17":"238401.80","18":"32110.03","19":"295439.81","20":"22601.15","21":"299378.97","22":"8681.99","23":"295439.81","24":"299378.97","25":"162.00","26":"260.00","27":"false","28":"true","29":"false","index":3},{"0":"W2_XL_input_clean_1004.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"343207","5":"James Williams","6":"037-07-1416","7":"95060 Crystal Burg","8":"Davisburgh","9":"AR","10":"30937-0798","11":"Frederick-Hendricks Group","12":"71-6374971","13":"01670 Adams Islands","14":"Angelatown","15":"AZ","16":"21706-7217","17":"249501.21","18":"26149.11","19":"254242.78","20":"19449.57","21":"224061.76","22":"6497.79","23":"254242.78","24":"224061.76","25":"164.00","26":"150.00","27":"true","28":"false","29":"false","index":4},{"0":"W2_XL_input_clean_1005.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"7480956","5":"Trevor Duncan","6":"768-05-9800","7":"41435 Hughes Drive","8":"New Teresaberg","9":"KS","10":"99528-9775","11":"White PLC Inc","12":"83-1015714","13":"None None","14":"Walker Landing Bakerfurt","15":"NJ","16":"21769","17":"168250.20","18":"53860.77","19":"159372.62","20":"12192.01","21":"206499.26","22":"5988.48","23":"159372.62","24":"206499.26","25":"149.00","26":"164.00","27":"false","28":"false","29":"false","index":5},{"0":"W2_XL_input_clean_1006.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"6307582","5":"Mary Dickerson","6":"036-91-4705","7":"95842 Freeman Coves","8":"Robertburgh","9":"MA","10":"63545-3314","11":"Morgan Ltd and Sons","12":"22-3437795","13":"147 Taylor Wall","14":"NULL","15":"NULL","16":"NULL","17":"237575.44","18":"85837.08","19":"278526.87","20":"21307.31","21":"182944.39","22":"5305.39","23":"278526.87","24":"182944.39","25":"123.00","26":"171.00","27":"false","28":"false","29":"false","index":6},{"0":"W2_XL_input_clean_1007.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"396234","5":"Seth Bailey","6":"760-19-8138","7":"341 Charles Mountains Morganberg","8":"NULL","9":"co","10":"47155-8501","11":"Brown LLC Group","12":"21-7657558","13":"None None","14":"NULL","15":"NULL","16":"NULL","17":"162805.19","18":"57402.86","19":"184235.06","20":"14093.98","21":"121322.24","22":"3518.34","23":"184235.06","24":"121322.24","25":"148.00","26":"119.00","27":"false","28":"true","29":"false","index":7},{"0":"W2_XL_input_clean_1008.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"4858295","5":"Robert Benson","6":"280-96-5377","7":"84813 Eric Way","8":"NULL","9":"NULL","10":"NULL","11":"Flores, Hubbard and Franklin Inc","12":"33-0179104","13":"638 Jennifer Springs","14":"Adamsmouth","15":"OR","16":"71848-7881","17":"71292.15","18":"22405.22","19":"59800.89","20":"4574.77","21":"65211.43","22":"1891.13","23":"59800.89","24":"65211.43","25":"300.00","26":"287.00","27":"false","28":"false","29":"true","index":8},{"0":"W2_XL_input_clean_1009.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"4458678","5":"Michelle Brown","6":"133-45-6693","7":"730 Huber Island","8":"Collinshaven","9":"IN","10":"29316-8972","11":"Cooper, Spence and Curry Ltd","12":"65-8736227","13":"871 Tonya Causeway New Davidborough AL 63538-3453","14":"NULL","15":"NULL","16":"NULL","17":"165296.64","18":"49693.12","19":"198300.45","20":"15169.98","21":"164615.68","22":"4773.85","23":"198300.45","24":"164615.68","25":"148.00","26":"245.00","27":"false","28":"true","29":"false","index":9},{"0":"W2_XL_input_clean_1010.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"3306669","5":"Bobby Torres","6":"592-52-8367","7":"9622 Jessica Estates","8":"Nathantown","9":"IA","10":"78428-1174","11":"Schaefer Group Ltd","12":"36-2682044","13":"18648 Costa Valleys","14":"78136-7800","15":"MI","16":"NULL","17":"52229.12","18":"7055.12","19":"65099.68","20":"4980.13","21":"62139.78","22":"1802.05","23":"65099.68","24":"62139.78","25":"195.00","26":"254.00","27":"true","28":"false","29":"false","index":10},{"0":"W2_XL_input_clean_1011.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"6208470","5":"Jill Case","6":"259-79-5957","7":"08470 Miller Stream","8":"Petersmouth","9":"IN","10":"55359-2536","11":"Young-Perkins PLC","12":"68-0385441","13":"03348 Parker Hills Garciaside NE","14":"NULL","15":"NULL","16":"06534-6570","17":"231779.84","18":"39340.33","19":"238491.85","20":"18244.63","21":"189412.80","22":"5492.97","23":"238491.85","24":"189412.80","25":"158.00","26":"223.00","27":"false","28":"false","29":"true","index":11},{"0":"W2_XL_input_clean_1012.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"3674217","5":"Joseph Miller","6":"246-23-8883","7":"68390 Blackwell Drive South","8":"Robert","9":"MD","10":"20384-0111","11":"Mitchell-Price Ltd","12":"91-4181453","13":"5154 Amanda Station","14":"Jessicastad","15":"MA","16":"10423-1803","17":"50379.86","18":"9384.61","19":"57145.11","20":"4371.60","21":"63120.83","22":"1830.50","23":"57145.11","24":"63120.83","25":"101.00","26":"110.00","27":"true","28":"false","29":"false","index":12},{"0":"W2_XL_input_clean_1013.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"3827600","5":"Jose Booker","6":"725-40-6045","7":"27374 Henderson Oval","8":"Lake James","9":"NJ","10":"32949-6377","11":"Mendoza-Matthews Group","12":"75-1618704","13":"712 Haley Meadow West","14":"Jeremymouth","15":"MD","16":"23850-8575","17":"242353.95","18":"66000.12","19":"191154.72","20":"14623.34","21":"200054.81","22":"5801.59","23":"191154.72","24":"200054.81","25":"216.00","26":"214.00","27":"false","28":"false","29":"true","index":13},{"0":"W2_XL_input_clean_1014.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"6605302","5":"Virginia Smith","6":"107-41-1388","7":"917 April Isle New","8":"Taylor","9":"IL","10":"20087-0048","11":"Bailey-Gross and Sons","12":"25-4904377","13":"2965 James Manors","14":"Christopherburgh","15":"OK","16":"23062-2861","17":"106876.37","18":"16645.06","19":"95686.59","20":"7320.02","21":"93749.00","22":"2718.72","23":"95686.59","24":"93749.00","25":"260.00","26":"248.00","27":"true","28":"false","29":"false","index":14},{"0":"W2_XL_input_clean_1015.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"2302462","5":"Aaron Swanson","6":"466-91-3126","7":"672 Brian Common Suite 123 East Lisa UT","8":"NULL","9":"NULL","10":"55739-1243","11":"Douglas LLC Ltd","12":"63-3760542","13":"84831 Hill Centers","14":"Port Christianberg","15":"KS","16":"62700-6312","17":"94560.78","18":"32659.11","19":"68248.81","20":"5221.03","21":"84120.42","22":"2439.49","23":"68248.81","24":"84120.42","25":"174.00","26":"281.00","27":"true","28":"false","29":"false","index":15},{"0":"W2_XL_input_clean_1016.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"5614245","5":"Laura Daniel","6":"051-22-2256","7":"73200 Tammy Bridge","8":"Chadfort","9":"ND","10":"32994-7451","11":"Jones-Jones Group","12":"42-1580203","13":"706 Cook Island","14":"Rubiotown","15":"IN","16":"25520-9318","17":"113671.47","18":"28250.59","19":"119797.41","20":"9164.50","21":"123417.38","22":"3579.10","23":"119797.41","24":"123417.38","25":"213.00","26":"234.00","27":"false","28":"false","29":"false","index":16},{"0":"W2_XL_input_clean_1017.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"4465330","5":"Kristi Johnson","6":"809-28-4999","7":"83396 Cross Pines","8":"Johnstonville","9":"SC","10":"94521-0987","11":"Thomas-Boyd Inc","12":"21-7956016","13":"0432 Taylor Square South","14":"Georgeton","15":"IN","16":"58391-0590","17":"232115.87","18":"24902.68","19":"163848.60","20":"12534.42","21":"299901.92","22":"8697.16","23":"163848.60","24":"299901.92","25":"279.00","26":"244.00","27":"true","28":"true","29":"false","index":17},{"0":"W2_XL_input_clean_1018.jpg","1":"W-2","2":"2010","3":"Copy B -- To Be Filed with Employee's FEDERAL Tax Return","4":"7029938","5":"Amy Cline","6":"536-93-9326","7":"8632 Mcclure Curve","8":"Gailfort","9":"NJ","10":"47117-3789","11":"Rangel, Wiggins and Ortega Ltd","12":"82-4064182","13":"3460 Stevenson Islands Sarahbury WY","14":"NULL","15":"NULL","16":"01038-0096","17":"157070.83","18":"37579.09","19":"135198.88","20":"10342.71","21":"153972.01","22":"4465.19","23":"135198.88","24":"153972.01","25":"276.00","26":"280.00","27":"false","28":"false","29":"false","index":18}],"schema":[{"key":"0","name":"scanned_filename","type":"string"},{"key":"1","name":"form_variant","type":"string"},{"key":"2","name":"tax_year","type":"string"},{"key":"3","name":"w2_copy","type":"string"},{"key":"4","name":"control_number","type":"string"},{"key":"5","name":"employee_name","type":"string"},{"key":"6","name":"employee_ssn","type":"string"},{"key":"7","name":"employee_street","type":"string"},{"key":"8","name":"employee_city","type":"string"},{"key":"9","name":"employee_state","type":"string"},{"key":"10","name":"employee_postal_code","type":"string"},{"key":"11","name":"employer_name","type":"string"},{"key":"12","name":"employer_id","type":"string"},{"key":"13","name":"employer_street","type":"string"},{"key":"14","name":"employer_city","type":"string"},{"key":"15","name":"employer_state","type":"string"},{"key":"16","name":"employer_postal_code","type":"string"},{"key":"17","name":"wages_tips","type":"decimal"},{"key":"18","name":"fed_income_tax_withheld","type":"decimal"},{"key":"19","name":"social_security_wages","type":"decimal"},{"key":"20","name":"social_security_tax_withheld","type":"decimal"},{"key":"21","name":"medicare_wages_tips","type":"decimal"},{"key":"22","name":"medicare_tax_withheld","type":"decimal"},{"key":"23","name":"social_security_tips","type":"decimal"},{"key":"24","name":"allocated_tips","type":"decimal"},{"key":"25","name":"non_qualified_plans","type":"decimal"},{"key":"26","name":"dependent_care_benefits","type":"decimal"},{"key":"27","name":"is_statutory_employee","type":"boolean"},{"key":"28","name":"is_retirement_plan","type":"boolean"},{"key":"29","name":"is_third_party_sick_pay","type":"boolean"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["17"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}}}},"trident":{"lakehouse":{"known_lakehouses":[{"id":"9d5afcfe-2300-4d7a-9937-3c652dfd1c97"}],"default_lakehouse":"9d5afcfe-2300-4d7a-9937-3c652dfd1c97","default_lakehouse_name":"AI_Demo_LH","default_lakehouse_workspace_id":"b26a43fd-5b25-48d9-b36e-bfb449977292"}}},"nbformat":4,"nbformat_minor":5}