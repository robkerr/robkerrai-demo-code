{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Libraries and Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries into kernel (if not already installed)\n",
    "# %pip install pinecone-client\n",
    "# %pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Python data handling environment imports \n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "# Pinecone is a cloud-based Vector Database we'll use \n",
    "# to store embeddings\n",
    "import pinecone\n",
    "\n",
    "# OpenAI is used for the embedding LLM and GenAI model \n",
    "# used to generate responses\n",
    "import openai\n",
    "\n",
    "# Langchain is middleware that ties together the components \n",
    "# of the embedding and retrieval pipelines \n",
    "\n",
    "# The embedding chain creates searchable vectors of our data\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "# A link in the chain to operate a chat session\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# We'll maintain some memory of the chat so follow-up questions\n",
    "# will be context-sensitive\n",
    "from langchain.chains.conversation.memory \\\n",
    "import ConversationBufferWindowMemory\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Environment Variables\n",
    "\n",
    "When using VSCode, install the dotenv extension and create an .env file with these contents:\n",
    "\n",
    "OPENAI_KEY=YOUR_OPENAI_API_KEY\n",
    "\n",
    "PINECONE_KEY=YOUR_PINECONE_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_KEY=os.getenv(\"OPENAI_KEY\")\n",
    "openai.api_key = OPENAI_KEY\n",
    "EMBEDDING_MODEL=\"text-embedding-ada-002\"\n",
    "GENAI_MODEL='gpt-3.5-turbo'\n",
    "\n",
    "PINECONE_KEY=os.getenv(\"PINECONE_KEY\")\n",
    "PINECONE_ENV=\"gcp-starter\"\n",
    "PINECONE_INDEX_NAME=\"default\" # this will be created below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://rhkdemo.blob.core.windows.net/demodata/squad-content.tsv\"\n",
    "df = pd.read_csv(URL, sep='\\t')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch only context knowledge about Detroit and London\n",
    "filtered_df = df.loc[df['subject'].isin(['Dell'])]\n",
    "print(filtered_df['subject'].value_counts())\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Pinecone Vector Database if does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(api_key = PINECONE_KEY, environment = PINECONE_ENV)\n",
    "index_list = pinecone.list_indexes()\n",
    "if len(index_list) == 0:\n",
    "    print(\"Creating index...\")\n",
    "    pinecone.create_index(PINECONE_INDEX_NAME, dimension=1536, metric='dotproduct')\n",
    "    \n",
    "print(pinecone.describe_index(PINECONE_INDEX_NAME))\n",
    "index = pinecone.Index(PINECONE_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embedding Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This references the text-embedding-ada-002 OpenAI model we'll use to create embeddings \n",
    "# Both for indexing ground knowledge content, and later when searching ground knowledge\n",
    "# For RAG documents to include in LLM Prompts\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model = EMBEDDING_MODEL,\n",
    "    openai_api_key= OPENAI_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a for loop to create embeddings for each of the Detroit & London knowledge articles, and \n",
    "# Then add the embeddings and orgiional article text to the vector databse\n",
    "# Shout-out to Dr. KM Moshin for this code snippet from his Excellent Udemy course on Pinecone!\n",
    "batch_size = 20 \n",
    "\n",
    "for i in tqdm(range(0, len(filtered_df), batch_size)):\n",
    "    # OpenAPI has rate limits, and we use batches to slow the pace of embedding requests\n",
    "    i_end = min(i+batch_size, len(filtered_df))\n",
    "    batch = filtered_df.iloc[i:i_end]\n",
    "    \n",
    "    # When querying the Vector DB for nearest vectors, the metadata \n",
    "    # is what is returned and added to the LLM Prompt (the \"Grounding Knowledge\")\n",
    "    meta_data = [{\"subject\" : row['subject'], \n",
    "              \"context\": row['context']} \n",
    "             for i, row in batch.iterrows()]\n",
    "    \n",
    "    # Get a list of documents to submit to OpenAI for embedding  \n",
    "    docs = batch['context'].tolist() \n",
    "    emb_vectors = embed.embed_documents(docs) \n",
    "\n",
    "    # The original ID keys are used as the PK in the Vector DB\n",
    "    ids = batch['id'].tolist()\n",
    "    \n",
    "    # Add embeddings, associated metadata, and the keys to the vector DB\n",
    "    to_upsert = zip(ids, emb_vectors, meta_data)    \n",
    "    index.upsert(vectors=to_upsert)\n",
    "    \n",
    "    # Pause for 10 seconds after each batch to avoid rate limits\n",
    "    time.sleep(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a simple query to the Vector Index to ensure we it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Pinecone(index, embed, \"context\")\n",
    "query = \"Who founded Dell?\" #ask some question that's answerable with the content added to the Vector DB\n",
    "vectorstore.similarity_search(query, k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a GPT 3.5 Turbo Chatbot with a 5 response memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reference to the OpenAI LLM\n",
    "llm = ChatOpenAI(openai_api_key = OPENAI_KEY,\n",
    "                model_name = GENAI_MODEL,\n",
    "                temperature = 0.0)\n",
    "\n",
    "# Ensure the chat session includes memory of 5 previous messages\n",
    "conv_mem = ConversationBufferWindowMemory(\n",
    "    memory_key = 'history',\n",
    "    k = 5,\n",
    "    return_messages =True)\n",
    "\n",
    "# Create the chain to manage the chat session\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\",\n",
    "    retriever = vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now have a conversation about the documents that were added to the grounding data vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.run(\"What do people like about london?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.run(\"is it expensive to live there?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.run(\"Does dell make surfboards?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.run(\"Do they make laptops?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.run(\"Who founded Dell computer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
